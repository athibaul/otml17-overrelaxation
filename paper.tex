\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\usepackage{cite}

% Math operators
\newcommand{\scal}[2]{\left\langle #1 , #2 \right\rangle}
\DeclareMathOperator{\IR}{\mathbb{R}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\One}{\mathbbm{1}}
\DeclareMathOperator{\Ccal}{\mathcal{C}}
\DeclareMathOperator{\logsumexp}{logsumexp}
\DeclareMathOperator{\diag}{diag}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\renewcommand{\epsilon}{\varepsilon}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}


\title{Over-relaxed Sinkhorn's Algorithm for Regularized Optimal Transport}


\author{
Alexis THIBAULT\\
\'Ecole Normale Sup\'erieure\\
Paris, France\\
\texttt{alexis.thibault@ens.fr}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
This article describes a method for quickly calculating the solution to the regularized optimal transport problem. It generalizes and improves upon the widely-used iterative Bregman projections algorithm (or Sinkhorn's algorithm). The idea is to over-relax the Bregman projection operators, allowing for faster convergence. In practice this corresponds to elevating the diagonal scaling factors to a given power, at each step of the algorithm.
\end{abstract}

\section{Introduction}
BLABLA to write


We consider two discrete probability measures $\mu^k \in \IR_{+*}^{n_k}$.
Let us define the two following linear operators
\begin{align*}
A_1 &: \begin{cases}
\IR^{n_1 n_2} \rightarrow \IR^{n_1} \\
(A_1 x)_i = \sum_j x_{i,j}
\end{cases} &
A_2 &: \begin{cases}
\IR^{n_1 n_2} \rightarrow \IR^{n_2}\\
(A_2 x)_j = \sum_i x_{i,j},
\end{cases}
\end{align*}
as well as thenlinear  constraint sets
\begin{align*}
\Ccal_k &= \left\{ \gamma\in\IR^{n_1 n_2} \mid A_k \gamma = \mu^k \right\}.
\end{align*}
Given a cost matrix $c$ where $c_{ij}$ represents the cost of moving mass $\mu^1_i$ to $\mu^2_j$,  the optimal transport problem corresponds to the following minimization problem:
$$\min_{\gamma\in\Ccal_1\cap \Ccal_2\cap \IR^{n_1 n_2}_+} \langle c,\gamma\rangle:=\sum_{i,j}c_{i,j}\gamma_{i,j}.$$
This is a linear programing problem whose resolution is intractable for large scale problems. 

\subsection{Regularized optimal transport}

In \cite{cuturi13}, it has been proposed to regularize this problem by adding a strictly convex entropy regularization:
\begin{equation}\label{ROT}
\min_{\gamma\in\Ccal_1\cap \Ccal_2\cap \IR^{n_1 n_2}_{+*}}K^\epsilon(\gamma) := \scal{c}{\gamma} 
+ \epsilon \sum_{i,j} \gamma_{i,j} (\log(\gamma_{i,j}) - 1)
,\end{equation}
with $\epsilon>0$. Considering, the Kullback-Leibler divergence:
\[ 
KL(\gamma,\xi) = \sum_{i,j} \gamma_{i,j} \left( \log \left( \frac{\gamma_{i,j}}{\xi_{i,j}} \right) -1  \right) + \sum_{i,j} \xi_{i,j},
\]
it was shown in \cite{benamou15}  that the unique minimizer of problem \eqref{ROT} writes
\begin{equation}\label{eq:reg_ot_pb}
\gamma^* = \argmin_{\Ccal_1 \cap \Ccal_2} K^\epsilon(\gamma^)= P_{\Ccal_1 \cap \Ccal_2} (e^{-c/\epsilon}),
\end{equation}
where $P_{\Ccal}$ is the  Bregman projection onto $\Ccal$ defined as:
\[
P_{\Ccal}(\xi) := \argmin_{\gamma \in \Ccal} KL(\gamma,\xi).
\]
The regularized  optimal transport plan $\gamma^*$ is thus the Bregman projection of $\gamma^0 = e^{-c/\epsilon}$ onto $\Ccal_1 \cap \Ccal_2$.


\subsection{Sinkhorn's algorithm}
Iterative Bregman projections onto $\Ccal_1$ and $\Ccal_2$ converge to a point in the intersection $\Ccal_1 \cap \Ccal_2$ \cite{bregman67}. Hence, the so-called Sinkhorn algorithm \cite{sinkhorn64} can be considered to compute the regularized  plan:
\begin{align*}
\gamma^0 &= e^{-c/\epsilon} &
\gamma^{l+1} = P_{\Ccal_2}(P_{\Ccal_1}(\gamma^l)),
\end{align*}
which checks 
$\lim_{l\rightarrow +\infty} \gamma^l = P_{\Ccal_1 \cap \Ccal_2}(\gamma^0) = \gamma^*.$

In the discrete setting,  projections actually correspond to diagonal scalings.
\begin{align*}
P_{\Ccal_1}(\gamma) &= \diag(a) \gamma &
a &=  \frac{\mu^1}{A_1 \gamma} \\
P_{\Ccal_2}(\gamma) &= \gamma \diag(b) &
b &= \frac{\mu^2}{A_2 \gamma}
\end{align*}
where the division is considered pointwise. 
To compute numerically the solution  one simply has to store $(a^l, b^l)\in\IR^{n_1+n2}$ and realize

\begin{align*}
a^{l+1} &= \frac{\mu^1}{\gamma^0 b^l} &
b^{l+1} &= \frac{\mu^2}{^t \gamma^0 a^{l+1}} 
\end{align*}
where the transport matrix can be recovered as :
$\gamma^l = \diag(a^l) \gamma^0 \diag(b^l).$ 
Efficient parallel computations can be considered \cite{cuturi13} and one can even reach real time computation for large scale problem for certain class of cost matrices $c$ \cite{Solomon2015}. 
For small values of the parameter $\epsilon$, numerical issues can arise and a stabilization of the algorithm is necessary \cite{2016arXiv160705816C}.
The convergence of the process can nevertheless be very slow  in the setting $\epsilon\to 0$ .
\subsection{Contributions}
In this paper, we consider an over-relaxation scheme  designed to accelerate  the Sinkhorn's algorithm. We first present and show the convergence of our algorithm in section 2. In section 3, we analyze the local convergence rate of the algorithm to justify the acceleration.
We finally demonstrate numerically  in Section 4 the good behavior of our method, where larger accelerations are observed for decreasing values of $\epsilon$.



\section{Over-relaxed Sinkhorn's algorithm}

\subsection{Over-relaxed projections}

Let us define the $\omega$-over-relaxed projection operator by:
\begin{equation}\label{eq:def_or_proj}
\log P^\omega_{\Ccal_k}(\gamma) = (1-\omega) \log \gamma + \omega \log P_{\Ccal_k}(\gamma),
\end{equation}
where the logarithm is taken coordinate-wise.
Note that $P_{\Ccal_k}^0 = Id$, $P_{\Ccal_k}^1 = P_{\Ccal_k}$, and $P_{\Ccal_k}^2$ is an involution (this is non-trivial, and uses the fact that $\Ccal_k$ is an affine subspace).

\subsection{Lyapunov function}
Let $\gamma^*$ denote the solution of the regularized OT problem.
The function $F$ is defined as:
\begin{equation}\label{eq:lyapunov_function}
F(\gamma) = KL(\gamma^*, \gamma)
\end{equation}
It shall be used as a Lyapunov function, so that $F(\gamma^{k+1}) < F(\gamma^k)$ as long as the process has not converged.


\begin{lemma} \label{lemma:KL_compact}
	For any $M \in \IR_+^*$, the sublevel set $\left\{ \gamma \mid F(\gamma) \le M \right\}$ is compact.
\end{lemma}
\begin{proof}
	{\color{red} TODO}
\end{proof}


Note that it is not necessary to know $\gamma^*$ in order to calculate the difference $F(\gamma) - F(P^\omega_{\Ccal_k}(\gamma))$, as shown by the following lemma.
\begin{lemma}\label{lemma:lyapunov_decrease}
	Take $\gamma$ in $\IR^{mn}_{+*}$. The decrease in value of the Lyapunov function can be calculated with the following formula:
	\begin{equation} \label{eq:kl_diff_scal}
	F(\gamma) - F(P^\omega_{\Ccal_k}(\gamma)) = 
	\scal{\mu^k}{\varphi_\omega \left(\frac{A_k \gamma}{\mu^k}\right)},
	\end{equation}
	where
	\begin{equation}
	\varphi_\omega(x) = x(1-x^{-\omega}) - \omega \log x
	\end{equation}
	is a real function, applied coordinate-wise.
\end{lemma}
Thus, calculating the decrease in value of the function $F$ is relatively cheap and straightforward.

\subsection{New algorithm}

\begin{theorem}
	Let $\theta$ and $\omega$ be such that $1\le \theta < \omega < 2$. Let $U$ be the open set defined by:
	\begin{equation}\label{eq:open_set_U}
	U = \left\{
	\gamma \in \IR_{+*}^{nm} \mid
	F(P^\omega_{\Ccal_1}(\gamma)) < F(\gamma)
	\text{ and }
	F(P^\omega_{\Ccal_2}(P^\theta_{\Ccal_1}(\gamma))) < F(P^\theta_{\Ccal_1}(\gamma))
	\right\}.
	\end{equation}
	Set the initial value $\gamma^0 = e^{-c/\epsilon}$, and iterate:
	\begin{align*}
	\gamma^{l+1} =
	\begin{cases}
	P^\theta_{\Ccal_2}(P^\theta_{\Ccal_1}(\gamma^l)) & \text{if } \gamma \in U \\
	P_{\Ccal_2}(P_{\Ccal_1})(\gamma^k) & \text{otherwise.}
	\end{cases}
	\end{align*}
	Then the iterates $(\gamma^l)$ converge to $\gamma^*$.
\end{theorem}
{\color{red} 
	In practice the test would be: check whether the $\omega$-relaxed projection of $\zeta$ on $\Ccal_1$ decreases the value of
	$F$; if so, calculate $P_{\Ccal_1}^\theta(\zeta)$ (don't update $\zeta$ yet) and check that its $\omega$-relaxed projection on $\Ccal_2$ decreases the value of $F$ again. \\
	$\longrightarrow$ Has to calculate 3 projections sometimes. This result is weaker than the one from my report, but easier to prove. Maybe we could avoid having $U$, and rather perform the test independently on each variable ? (Or we could find a cheaper test).}
\begin{remark}
	Given lemma \ref{lemma:lyapunov_decrease}, it can be seen that $\gamma^* \in U$. Therefore, iterations eventually all use the over-relaxed operators.
\end{remark}

\begin{lemma}
	\label{lemma:trivial_intersection}
	Let us take $\gamma^0$ in $\IR_{+*}^{n_1 n_2}$,
	and denote
	\[
	S = \left\{
	\diag(a) \gamma^0 \diag(b),\quad
	(a,b) \in \IR_{+*}^{n_1 + n_2}
	\right\}
	\]
	the set of matrices that are diagonally similar to $\gamma^0$.
	Then the set $S \cap \Ccal_1 \cap \Ccal_2$ contains exactly one element $\gamma^* = P_{\Ccal_1 \cap \Ccal_2}(\gamma^0)$.
\end{lemma}

\begin{lemma}\label{lemma:F_P_theta}
	Let $1\le \theta < \omega$. Then, for any $\gamma \in \IR_{+*}^{nm}$, one has
	\begin{equation}\label{eq:F_P_theta}
	F(P^\theta_{\Ccal_k}(\gamma)) \le F(P^\omega_{\Ccal_k}(\gamma)).
	\end{equation}
	Moreover, equality occurs if and only if $\gamma \in \Ccal_k$.
\end{lemma}
\begin{proof}
	Thanks to lemma \ref{lemma:lyapunov_decrease}, one knows that
	\[
	F(P^\theta_{\Ccal_k}(\gamma)) - F(P^\omega_{\Ccal_k}(\gamma))
	= \scal{\mu^k}{(\varphi_\omega - \varphi_\theta) \left( \frac{A_k \gamma}{\mu^k} \right) } .
	\]
	Moreover, $t \in [1,\infty) \mapsto \varphi_t(x)$ is nonincreasing:
	\[
	\frac{d}{dt} \varphi_t(x) = \log x (x^{1-t} - 1).
	\]
	For $x\neq 1$, it is even strictly decreasing.
	Thus the inequality (\ref{eq:F_P_theta}) is valid, with equality \emph{iff} $A_1 \gamma = \mu$.
\end{proof}

\begin{proof}[Proof of the theorem]
	(New proof, using the Lyapunov function. Much shorter and cleaner than the one from my report.)
	First of all, notice that the operators $P_{\Ccal_k}^\theta$ apply a scaling to lines or columns of matrices. All $(\gamma^l)$ are thus diagonally similar to $\gamma^0$:
	\[
	\forall l\ge0,\quad \gamma^l \in S
	\]
	
	The set $U$ is constructed in such a way that, by applying lemma \ref{lemma:F_P_theta} several times, one ensures that the sequence $(F(\gamma^l))$ is nonincreasing. Moreover, it is strictly decreasing as long as $\gamma^l \not \in \Ccal_1 \cap \Ccal_2$.
	Hence, 
	\[
	F(\gamma^l) \le F(\gamma^0).
	\]
	By lemma \ref{lemma:KL_compact}, the sequence $(\gamma^l)$ is thus precompact. Let us show that any accumulation point $\zeta$ belongs to $\Ccal_1 \cap \Ccal_2$. Since $S$ is closed in $\IR_{+*}^{n_1 n_2}$, by lemma \ref{lemma:trivial_intersection} this would mean that $\zeta = \gamma^*$, and would be sufficient to prove the convergence.
	
	Let $(\gamma^{k_l})$ be a subsequence of $(\gamma^l)$ that converges to $\zeta$.
	Let us denote, for convenience, $T_1 = P_{\Ccal_2}^\theta \circ P_{\Ccal_1}^\theta$ and $T_2 = P_{\Ccal_2} \circ P_{\Ccal_1}$ the two operators used in the iteration, and $T$ the compound operator, so that $T_{|U} \equiv T_{1|U}$ and $T_{|U^c} \equiv T_{2|U^c}$.
	\begin{itemize}
		\item If $\zeta \in U$, then since $U$ is open, all $(\gamma^{k_l})$ are eventually in $U$ as well.
		Thus, for any large enough $l$, $ T(\gamma^{k_l}) = T_1(\gamma^{k_l}) $. By continuity of $T_1$ on $U$, since $(\gamma^{k_l})$ converges to $\zeta$, 
		\[T(\zeta) = \lim_{l\rightarrow \infty} T(\gamma^{k_l}).\] Thus
		 \[F(T(\zeta)) = \lim_{l \rightarrow \infty} F(T(\gamma^{k_l})) \ge \lim_{l\rightarrow \infty} F(\gamma^{k_{l+1}}) .\]
		 \[
		 F(T(\zeta)) \ge F(\zeta)
		 \] Since $\zeta$ is inside the set $U$, which ensures strict decrease of the Lyapunov function, this is absurd.
		 \item If $\zeta \in U^c$ and an infinite number of $(\gamma^{k_l})$ are in $U^c$, then the same argument of continuity applied to $T_2$ yields
		 \[
		 F(T(\zeta)) = F(\zeta).
		 \]
		 The function $\varphi_1$ from lemma \ref{lemma:lyapunov_decrease} is strictly positive except at $x=1$. Since the case of equality occurs for both projections, this ensures that $\zeta \in \Ccal_1 \cap \Ccal_2$.
		 
		 \item If $\zeta \in U^c$ and all but finitely many $\gamma^{k_l}$ are in $U$, then $\zeta$ is on the border $\partial U$. In particular it belongs to $\bar{U}$, and therefore
		 \[
		 F(P^\omega_{\Ccal_1}(\zeta)) \le F(\zeta) \quad
		 \text{ and } \quad
		 F(P^\omega_{\Ccal_2}(P^\theta_{\Ccal_1}(\zeta))) \le F(P^\theta_{\Ccal_1}(\zeta)).
		 \]
		 By lemma \ref{lemma:F_P_theta}, the inequality \begin{equation} \label{eq:ineq1}
		 F(P_{\Ccal_1}^\theta(\zeta)) \le F(P_{\Ccal_1}^\omega(\zeta)) \end{equation}
		 is valid; it is strict unless $\zeta \in \Ccal_1$.
		 The same lemma yields
		 \begin{equation} \label{eq:ineq2}
		 F(P_{\Ccal_2}^\theta(P_{\Ccal_1}^\theta(\zeta)) \le F(P_{\Ccal_2}^\omega(P_{\Ccal_1}^\theta(\zeta)), \end{equation}
		 with a strict inequality unless $P_{\Ccal_1}^\theta(\zeta) \in \Ccal_2$.
		 To sum up, we have the series of inequalities:
		 \[
		 F(\zeta) 
		 \overset{\zeta \in \bar{U}}{\ge}
		 F(P^\omega_{\Ccal_1}(\zeta))
		 \overset{\text{(\ref{eq:ineq1})}}{\ge}
		 F(P^\theta_{\Ccal_1}(\zeta))
		 \overset{\zeta \in U}{\ge}
		 F(P_{\Ccal_2}^\omega(P_{\Ccal_1}^\theta(\zeta)))
		 \overset{\text{(\ref{eq:ineq2})}}{\ge}
		 F(P_{\Ccal_2}^\theta(P_{\Ccal_1}^\theta(\zeta)))
		 \]
		 
		 Yet, by continuity of $T_1 = P_{\Ccal_2}^\theta \circ P_{\Ccal_1}^\theta $ on $\bar{U}$, we have
		 \[
		 F(P_{\Ccal_2}^\theta(P_{\Ccal_1}^\theta(\zeta))) = F(\zeta).
		 \]
		 Thus both (\ref{eq:ineq1}) and (\ref{eq:ineq2}) fall in the case of equality, which means $\zeta \in \Ccal_1$ and $P^\theta_{\Ccal_1}(\zeta) = \zeta \in \Ccal_2$.
	\end{itemize}
\end{proof}


\section{Local rate of convergence ?}

\section{Experimental results}
{\color{red} TODO}


\section*{Acknowledgments}


%\subsubsection*{References}

\bibliographystyle{apalike}
\bibliography{references}

\end{document}
