\documentclass[compress]{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xcolor} % Pour colorer des symboles dans les Ã©quations
\usepackage{bbm}

\usepackage{tikz}
\usepackage{cite}

% Math operators
\newcommand{\scal}[2]{\left\langle #1 , #2 \right\rangle}
\DeclareMathOperator{\IR}{\mathbb{R}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\One}{\mathbbm{1}}
\DeclareMathOperator{\Ccal}{\mathcal{C}}
\DeclareMathOperator{\logsumexp}{logsumexp}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\KL}{KL}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\renewcommand{\epsilon}{\varepsilon}

\newtheorem{proposition}{Proposition}

\title[Overrelaxed SK Algorithm]{Overrelaxed Sinkhorn--Knopp Algorithm for Regularized Optimal Transport}

%
\author[A. THIBAULT]{
	Alexis THIBAULT, 
	L\'ena\"ic CHIZAT,\\
	Charles DOSSAL,
	Nicolas PAPADAKIS
}
\institute[OTML'17]{Optimal Transport and Machine Learning workshop \\ NIPS 2017}
\date{December 9, 2017}

\usetheme{CambridgeUS}


\begin{document}

\titlepage

\begin{frame}
{Context} 
\begin{itemize}
\item Fast computation of regularized Optimal Transport with the Sinkhorn--Knopp algorithm (SK) {\color{blue}[Cuturi, 2013]}
\item For small regularization $\epsilon$, more precise but slower
\end{itemize}
\begin{tabular}{c c c c}
	\includegraphics[height=3cm]{1d_interp_mu} &
	\includegraphics[height=3cm]{1d_interp_0_2} &
	\includegraphics[height=3cm]{1d_interp_2} &
	\includegraphics[height=3cm]{1d_interp_18} \\
	&
	$\epsilon = 5$ &
	$\epsilon = 0.5$ &
	$\epsilon = 0.02$
\end{tabular}
\end{frame}


\begin{frame}
	\tableofcontents
% We'll be looking at a way of accelerating an algorithm, called the SK algorithm, whose goal is to calculate the solution of the regularized optimal transport problem. The principle of overrelaxation has been shown to successfully accelerate this algorithm. However, there is no proof in the literature that the proposed algorithm always converges, and no automated choice of parameters.
% In our paper, we suggest a way of ensuring global convergence. This is done simply by choosing the overrelaxation parameters at each step of the algorithm so that a given function decreases. We will motivate our choice of function with numerical results.
\end{frame}

\section[Overrelaxing SK]{Acceleration of  the Sinkhorn--Knopp algorithm }

\subsection[SK algorithm]{The Sinkhorn--Knopp algorithm}
\begin{frame}{Regularized optimal transport}
Entropic regularization of optimal transport:
\begin{equation*} \label{eq:problem}
\gamma^* = \argmin_{\gamma \in \Ccal_1 \cap \Ccal_2}
	\scal{c}{\gamma} + \epsilon \KL(\gamma,\One)
\end{equation*}
$c \in \IR_+^{n_1 n_2}$ cost matrix, $\mu_1 \in \IR^{n_1}$, $\mu_2 \in \IR^{n_2}$ discrete measures, $\epsilon > 0$ regularization .

\pause
Constraint sets:
\begin{align*}
\Ccal_1 &= \left\{ \gamma \mid A_1 \gamma = \mu^1 \right\},
&
\Ccal_2 &= \left\{ \gamma \mid A_2 \gamma = \mu^2 \right\}.
\end{align*}
\pause
Kullback--Leibler divergence:
\begin{equation*}\label{KL}
\KL(\gamma,\xi) = \sum_{i,j} \gamma_{i,j} \left( \log \left( \frac{\gamma_{i,j}}{\xi_{i,j}} \right) -1  \right) + \sum_{i,j} \xi_{i,j}
\end{equation*}
\end{frame}


\begin{frame}{Bregman projections}
	
Bregman projection onto $\Ccal$:
\[
P_{\Ccal}(\xi) := \argmin_{\gamma \in \Ccal} \KL(\gamma,\xi).
\]
\pause
Setting $\gamma^0 = e^{-c/\epsilon}$:
\begin{align*}
\gamma^* &= \argmin_{\gamma \in \Ccal_1 \cap \Ccal_2}
	\scal{c}{\gamma} + \epsilon \KL(\gamma,\One)
& &\Longleftrightarrow &
\gamma^* &= P_{\Ccal_1 \cap \Ccal_2}(\gamma^0)
\end{align*}
\pause
Bregman projection onto $\Ccal_1$ or $\Ccal_2$ $\Longleftrightarrow$ scaling of rows or columns
\begin{align*}\label{scaling}
P_{\Ccal_1}(\gamma) &= \diag(a) \gamma &\text{with}\quad
a &=  {\mu^1}\oslash{A_1 \gamma}, \\
P_{\Ccal_2}(\gamma) &= \gamma \diag(b) &\text{with}\quad
b &= {\mu^2}\oslash{A_2 \gamma},\nonumber
\end{align*}
where $\oslash$ is the entry-wise division.
\end{frame}

\begin{frame}{Sinkhorn--Knopp algorithm}
	
	\begin{columns}
	\begin{column}{0.6\textwidth}

	Sinkhorn--Knopp algorithm (SK):
	\[
	\lim P_{\Ccal_2}\circ P_{\Ccal_1} \circ \ldots \circ P_{\Ccal_2} \circ P_{\Ccal_1} (\gamma^0) = \gamma^*
	\]
	
	{\color{blue} [Benamou et al., 2015]}
	
\end{column}

\begin{column}{0.4\textwidth}
	\centering
	\input{schema_a.tex}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Limitation of the SK algorithm}
Linear convergence:
\[ \norm{\gamma^\ell - \gamma^*} = \mathcal{O}\left((1-\eta)^\ell \right) , \quad \eta > 0 \]

\begin{columns}
	\begin{column}{0.5\textwidth}
		\centering
		\input{schema_a.tex}\\
		Large $\epsilon$
	\end{column}
	\begin{column}{0.5\textwidth}
		\centering
		\input{schema_b.tex}\\
		Small $\epsilon$
	\end{column}
\end{columns}

Small $\epsilon$ $\Longrightarrow$
very small $\eta$ $\Longrightarrow$
slow convergence
\end{frame}

\subsection{Overrelaxation}
\begin{frame}{Overrelaxation}
	\begin{columns}
		\begin{column}{0.6\textwidth}
			Overrelaxed projections of parameter $\theta \in \IR$:
			\begin{align*}\label{or_scaling}
			P_{\Ccal_1}^\theta(\gamma) &= \diag(a)^\theta \gamma\\
			P_{\Ccal_2}^\theta(\gamma) &= \gamma \diag(b)^\theta \nonumber
			\end{align*}
			(element-wise exponentiation)
		\end{column}
		\begin{column}{0.4\textwidth}
			\centering
			\input{schema_c.tex}\\
			Typically take $\theta \in [1,2)$.
		\end{column}
	\end{columns}
\end{frame}



\section{Ensuring global convergence}

\iffalse
\subsection[Local rate]{Local rate of convergence}

\begin{frame}{Local study}
		Overrelaxation modifies each eigenvalue of $D_{\gamma^*}(P^\theta_{\Ccal_1} \circ P^\theta_{\Ccal_2})$.
		\begin{center}
			\includegraphics[width=6cm]{images/eigen_transform.png}
		\end{center}
		\pause
		Optimal parameter: $\theta^* = \frac{2}{1+\sqrt{\eta}}$\\
		\pause
		Linear convergence rate: $\frac{1-\sqrt{\eta}}{1+\sqrt{\eta}} \sim 1-2\sqrt{\eta}$
\end{frame}
\fi



\begin{frame}{Algorithm}
How to ensure global convergence?
\pause
\begin{proposition}
Choose $F(\gamma)$ continuous, coercive function whose unique minimizer is $\gamma^*$.\\
Choose $\theta_1(\gamma)$, $\theta_2(\gamma)$ continuous functions, such that
\begin{equation*}\label{eq:cond_theta_k}
\forall k \in \{1,2\}, \, \forall \gamma \notin \Ccal_k,\quad\quad
F(P_{\Ccal_k}^{\theta_k(\gamma)}(\gamma)) < F(\gamma).
\end{equation*}
\pause
Alternately iterate $P_{\Ccal_1}^{\theta_1}$ and $P_{\Ccal_2}^{\theta_2}$, starting from $\gamma^0 = e^{-c/\epsilon}$.\\
This process converges to $\gamma^*$.
\end{proposition}
\end{frame}

\subsection{Lyapunov function}
\begin{frame}{Ensuring global convergence}
	Our choice of Lyapunov function:
	\[F(\gamma) := \KL(\gamma^*,\gamma)\]
	\pause
	
	Motivation of this arbitrary choice:
	\begin{itemize}
		\item Makes sense
		% Verifies properties
		% + In the Bregman context, it is not absurd to consider this function. Decreases when performing simple Bregman projections.
		\item Easy calculation of differences % between the evaluation on a point and on its projection
		\item Decreases for \emph{most} $\theta \in [1,2)$ in practice % Allows to choose rather large overrelaxation parameters
	\end{itemize}
	
\end{frame}

\begin{frame}{Properties of the Lyapunov function}
Calculating differences: for $k \in \{1,2\}$,
\begin{equation*} \label{eq:kl_diff_scal}
F(\gamma) - F(P^\theta_{\Ccal_k}(\gamma)) = 
\scal{\mu^k}{\varphi_\theta \left((A_k \gamma) \oslash \mu^k \right)},
\end{equation*}
where
\begin{equation*}
\varphi_\theta(x) = x(1-x^{-\theta}) - \theta \log x
\end{equation*}
is a real function, applied coordinate-wise.
\pause
\begin{block}{Sufficient condition}
	If every coordinate of $\varphi_\theta \left((A_k \gamma) \oslash \mu^k \right)$ is strictly positive, then $F$ decreases.
\end{block}
\end{frame}

\subsection[Choice of OR parameters]{Choice of overrelaxation parameters}
\begin{frame}{Ensuring the decrease}
\begin{block}{Sufficient condition}
	If every coordinate of $\varphi_\theta \left((A_k \gamma) \oslash \mu^k \right)$ is strictly positive, then $F$ decreases.
\end{block}
\begin{center}
	\vspace*{-0.2cm}\includegraphics[height=5cm]{images/cvgce_zone_2.png}\vspace{-0.2cm}\\
	{\small \em Function $\varphi_\theta(x)$ is positive above the red line, negative below.}
\end{center}
\end{frame}

\begin{frame}{Advantages of this choice}
	\begin{align*}
	\Theta^*(u) &:= \max \left\{\theta \mid \varphi_\theta(u) \ge 0 \right\}
	&
	\Theta(u) &:= clip_{[1,\theta_0]}(\Theta^*(\min u)-\delta)
	\end{align*}
	\[
	k \in \{1,2\}, \quad \theta_k (\gamma) := \Theta((A_k \gamma) \oslash \mu^k)
	\]
	
	\begin{block}{Remark}
		The vector $(A_k \gamma)\oslash \mu^k$ is calculated once whenever performing a projection.
	\end{block}
	\pause
	\begin{itemize}
		\item Easy calculations (1-D monotone problem for $\Theta^*$)
		\item Excellent choice in practice
		\item Possibility to choose the final parameter $\theta_0$
	\end{itemize}
	\pause
	\begin{block}{Optimal parameter}
		If SK converges at rate $1-\eta$, then $\theta_0 = \frac{2}{1+\sqrt{\eta}}$ is asymptotically optimal.
	\end{block}
\end{frame}

\section{Numerical results}
\begin{frame}{Numerical results}
	\begin{itemize}
	\item Speed comparison with SK, on problems of size $100 \times 100$, for various~$\epsilon$\\
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/speedratio_image}\\
			\footnotesize Euclidean 1-D cost, random marginals
		\end{column}
		\begin{column}{0.5\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/speedratio_ML}\\
			\footnotesize  Random cost, uniform marginals
		\end{column}
	\end{columns}
	\vspace{0.2cm}
	\item About 2 to 30 times faster in most cases
	\end{itemize}
\end{frame}


\section*{Conclusion}
\begin{frame}{Conclusion}
	\begin{itemize}
		\item Overrelaxation
		\item Global convergence
		\item Adaptive choice of parameters
		\item Efficient, especially for small $\epsilon$
		\item Generalizable (multi-marginal, barycenters)
	\end{itemize}
\end{frame}


\end{document}
