%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% baposter Landscape Poster
% LaTeX Template
% Version 1.0 (11/06/13)
%
% baposter Class Created by:
% Brian Amberg (baposter@brian-amberg.de)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[landscape,a0paper,fontscale=0.35]{baposter} % Adjust the font scale/size here : fontscale=0.285
%\usepackage{textpos}
\usepackage[frenchb]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{graphicx} % Required for including images
%\graphicspath{{Figures/}} % Directory in which figures are stored
\usepackage{amsmath} % For typesetting math
\usepackage{amsthm}
\usepackage{amssymb} % Adds new symbols to be used in math mode
\usepackage{booktabs} % Top and bottom rules for tables
\usepackage{enumitem} % Used to reduce itemize/enumerate spacing
\usepackage{palatino} % Use the Palatino font
\usepackage[font=small,labelfont=bf]{caption} % Required for specifying captions to tables and figures
\usepackage{multicol} % Required for multiple columns
\usepackage{tikz} % Required for flow chart
\usetikzlibrary{shapes,arrows} % Tikz libraries required for the flow chart in the template
%\usepackage{dsfont}
%\usepackage{listings}
\usepackage{subfigure}
\usepackage{color}
%\usepackage{nth}
%\usepackage{a4wide}
\usepackage{graphicx,color}
\usepackage{array}
%\usepackage[nottoc, notlof, notlot]{tocbibind}
\usepackage{subfigure}
\usetikzlibrary{arrows,shapes,positioning,calc}
%\definecolor{CPU}{RGB}{30, 167, 223}
%\definecolor{CPU}{RGB}{8, 138, 104}
\definecolor{gris}{gray}{0.9}
\usepackage{geometry}
\geometry{left=0.9cm,right=0.9cm,bottom=0.9cm,top=0.9cm,centering}
%\usepackage{subfig}
\usepackage{float}
\usepackage{multicol}
\usepackage{enumitem,rotating}
\usepackage{bbm}
\usepackage{tcolorbox}


\newcommand{\sidecap}[1]{ {\begin{sideways}\parbox{0.2\textwidth}{\centering #1}\end{sideways}} }

\theoremstyle{plain}
\newtheorem*{thm}{Theorem}
%\theoremstyle{definition}
%\newtheorem*{defi}[thm]{Definition}
\theoremstyle{plain}
\newtheorem*{prop}{Properties}
\theoremstyle{plain}
\newtheorem*{hyp}{Assumption}
\theoremstyle{plain}
\newtheorem*{proposition}{Proposition}

\setlength{\columnsep}{1.5em} % Slightly increase the space between columns
\setlength{\columnseprule}{0mm} % No horizontal rule between columns

\definecolor{lightred}{rgb}{0.5 0 0}

\newcommand{\compresslist}{ % Define a command to reduce spacing within itemize/enumerate environments, this is used right after \begin{itemize} or \begin{enumerate}
\setlength{\itemsep}{1pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
}

\definecolor{CPU}{RGB}{0, 153, 125} % Defines the color used for content box headers




% Math operators
\newcommand{\scal}[2]{\left\langle #1 , #2 \right\rangle}
\DeclareMathOperator{\IR}{\mathbb{R}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\One}{\mathbbm{1}}
\DeclareMathOperator{\Ccal}{\mathcal{C}}
\DeclareMathOperator{\logsumexp}{logsumexp}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\KL}{KL}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\renewcommand{\epsilon}{\varepsilon}


\begin{document}

\begin{poster}
{
background=plain,
headershade=plain,
headerborder=closed, % Adds a border around the header of content boxes
colspacing=0.8em, % Column spacing
bgColorOne=gris, % Background color for the gradient on the left side of the poster
bgColorTwo=gris, % Background color for the gradient on the right side of the poster
borderColor=CPU, % Border color
headerColorOne=CPU, % Background color for the header in the content boxes (left side)
headerColorTwo=CPU, % Background color for the header in the content boxes (right side)
headerFontColor=white, % Text color for the header text in the content boxes
boxColorOne=white, % Background color of the content boxes
textborder=roundedsmall, % Format of the border around content boxes, can be: none, bars, coils, triangles, rectangle, rounded, roundedsmall, roundedright or faded
eyecatcher=true, % Set to false for ignoring the left logo in the title and move the title left
headerheight=0.10\textheight, % Height of the header
headershape=rounded, % Specify the rounded corner in the content box headers, can be: rectangle, small-rounded, roundedright, roundedleft or rounded
headerfont=\Large\bf\textsc, % Large, bold and sans serif font in the headers of content boxes
%textfont={\setlength{\parindent}{1.5em}}, % Uncomment for paragraph indentation
linewidth=1.8pt, % Width of the border lines around content boxes
columns=3,
}
%----------------------------------------------------------------------------------------
%	TITLE SECTION 
%----------------------------------------------------------------------------------------
%
{\includegraphics[height=6em]{logo_ens_psl.png}} % First university/lab logo on the left
{\bf Overrelaxed Sinkhorn--Knopp algorithm\\ for regularized optimal transport\vspace{0.2em}} 
{{Alexis Thibault, L\'ena√Øc Chizat, Charles Dossal and  Nicolas Papadakis}} % Author names and institution
{\includegraphics[height=6em]{logo_imb.png}} % Second university/lab logo on the right

%----------------------------------------------------------------------------------------
%	RESEARCH
%----------------------------------------------------------------------------------------

% Introduction

\headerbox{Introduction}{name=intro,column=0,span=2}{
	\begin{minipage}{0.55\textwidth}
		\begin{itemize}
			\item[] Optimal Transport is an efficient and flexible tool to compare two probability distributions. Thanks to entropic regularization [Cuturi, 2013], the fast Sinkhorn--Knopp algorithm (SK) [Sinkhorn, 1964] can be used to numerically calculate the solution.
			
			\item[] Yet, for small regularization $\epsilon$, the SK algorithm becomes extremely slow. To counter this, we propose a new algorithm that accelerates SK by \emph{overrelaxation}.
			This idea has been numerically shown to accelerate the algorithm, but with no proof of convergence.
			We ensure global convergence by choosing the overrelaxation parameters at each step of the algorithm so that a given Lyapunov function decreases.
			
			\item[] The algorithm we obtain converges between 2 and 20 times faster than Sinkhorn--Knopp, while having almost the same cost per iteration, and retaining other nice properties.
		\end{itemize}
	\end{minipage}
	\begin{minipage}{0.45\textwidth}
		\centering
		\begin{tcolorbox}[width=0.99\textwidth,/tcb/size=small, /tcb/top=2.0mm]
			\centering
			\begin{tabular}{c c c c}
				\includegraphics[height=3cm]{1d_interp_mu} &
				\includegraphics[height=3cm]{1d_interp_0_2} &
				\includegraphics[height=3cm]{1d_interp_2} &
				\includegraphics[height=3cm]{1d_interp_18} \\
				&
				$\epsilon = 5$ &
				$\epsilon = 0.5$ &
				$\epsilon = 0.02$
			\end{tabular}
			\captionof{figure}{Regularized optimal transport maps for decreasing regularization parameter $\epsilon$.} \label{fig:1d_ot}
		\end{tcolorbox}
	\end{minipage}
}




%----------------------------------------------------------------------------------------
%	
%----------------------------------------------------------------------------------------

\headerbox{Overrelaxation}{name=overrelaxation,column=1,below=intro}{
\begin{minipage}{0.49\textwidth}
	\begin{tcolorbox}[size=small, top=2mm]
		\centering
		\begin{tabular}{c c}
		\input{schema_b} &
		\input{schema_c} \\
		SK &
		Overrelaxed SK
		\end{tabular}
	\captionof{figure}{For small regularization $\epsilon$, the SK algorithm converges slowly, whereas the overrelaxed SK algorithm converges with a better rate}
	\end{tcolorbox}
\end{minipage}
\begin{minipage}{0.49\textwidth}
	\vspace{0.6mm}
	Although the SK algorithm is much more efficient than linear solvers in a high-dimensional setting, it becomes extremely slow when the regularization $\epsilon$ is small.
	To counteract this limitation, we use the principle of \emph{overrelaxation}.
	
	We define the overrelaxed projections of parameter $\theta \in \IR$:
	\begin{align*}\label{or_scaling}
	P_{\Ccal_1}^\theta(\gamma) &= \diag(a)^\theta \gamma\\
	P_{\Ccal_2}^\theta(\gamma) &= \gamma \diag(b)^\theta \nonumber .
	\end{align*}
	In these equations, the exponentiation is applied element-wise.
\end{minipage}
As in the Sinkhorn--Knopp algorithm, we iterate these operators alternately.
If we consider the linearization of the operators around the solution, iterative overrelaxed projections can be shown to converge for any $\theta \in (0,2)$.
In the global, non-euclidean setting, we use an additional criterion based on a Lyapunov function.
}





%----------------------------------------------------------------------------------------
% Main result
%----------------------------------------------------------------------------------------

\headerbox{Lyapunov Function}{name=lyapunov,column=2}{
	To ensure convergence of iterative overrelaxed Bregman projections, we assert the decrease of a Lyapunov function.
	\begin{tcolorbox}[size=small]
    	Choose $F(\gamma)$ continuous, coercive function whose unique minimizer is $\gamma^*$.\\
    	Choose $\theta_1(\gamma)$, $\theta_2(\gamma)$ continuous functions, such that
    	\begin{equation*}\label{eq:cond_theta_k}
    	\forall k \in \{1,2\}, \, \forall \gamma \notin \Ccal_k,\quad\quad
    	F(P_{\Ccal_k}^{\theta_k(\gamma)}(\gamma)) < F(\gamma).
    	\end{equation*}
    	Alternately iterate $P_{\Ccal_1}^{\theta_1}$ and $P_{\Ccal_2}^{\theta_2}$, starting from $\gamma^0 = e^{-c/\epsilon}$.
    	This process converges to $\gamma^*$.
    \end{tcolorbox}
    Our choice of Lyapunov function is: $F(\gamma) := \KL(\gamma^*,\gamma)$
    Note that this function decreases for regular Bregman projections.
    The difference between two evaluations can be computed directly:
    
    \begin{minipage}{0.65\textwidth}
    	\begin{equation*} \label{eq:kl_diff_scal}
    	F(\gamma) - F(P^\theta_{\Ccal_k}(\gamma)) = 
    	\scal{\mu^k}{\varphi_\theta \left((A_k \gamma) \oslash \mu^k \right)}.
    	\end{equation*}
    	We want this difference to be positive. A sufficient condition is that every coordinate of $\varphi_\theta \left((A_k \gamma) \oslash \mu^k \right)$ be positive. Functions $\theta_1, \theta_2$ are crafted for this purpose.
    	\begin{gather*}
    	\Theta^*(u) := \max \left\{\theta \mid \varphi_\theta(u) \ge 0 \right\}
    	\\
    	\Theta(u) := clip_{[1,\theta_0]}(\Theta^*(\min u)-\delta)\\
    	k \in \{1,2\}, \quad \theta_k (\gamma) := \Theta((A_k \gamma) \oslash \mu^k)
    	\end{gather*}
    \end{minipage}
	\begin{minipage}{0.34\textwidth}
		\begin{tcolorbox}[size=small]
			\centering
			\includegraphics[width=\textwidth]{images/cvgce_zone_2.png}
			\captionof{figure}{
				Function $\varphi_\theta(x)$ is positive above the red line, negative below.}
		\end{tcolorbox}
	\end{minipage}
This adaptive choice allows to choose the overrelaxation parameter $\theta_0$ that is used asymptotically. A linear study around the solution shows that overrelaxation modifies the eigenvalues in a predictable way: in particular, if the linear convergence rate of the Sinkhorn--Knopp algorithm is $1-\eta$, then the optimal asymptotic overrelaxation parameter is $\theta_0 = \frac{2}{1+\sqrt{\eta}}$.
}



%----------------------------------------------------------------------------------------
%	Bootstrap
%----------------------------------------------------------------------------------------

\headerbox{Experiments}{name=expe,below=overrelaxation,column=1,span=2}{
	\begin{minipage}{0.32\textwidth}
		We compare our algorithm to SK algorithm on two different settings of size $100 \times 100$.
		\begin{itemize}
			\item[(a)] Cost $c$ is the squared euclidean norm on $[0,1]$. Marginals are randomly generated, piecewise constant.
			\item[(b)] Cost $c$ is a $100\times 100$ random matrix with entries uniform in $[0,1]$. Marginals are uniform.
		\end{itemize}
		In both experimental settings, our algorithm wins by a factor above $20$ for small $\epsilon$.
	\end{minipage}
	\begin{minipage}{0.68\textwidth}
		\begin{tcolorbox}[size=small]
			\begin{minipage}{0.5\textwidth}
				\centering
				\includegraphics[width=\textwidth]{images/speedratio_image}
			\end{minipage}%
			\begin{minipage}{0.5\textwidth}
				\centering
				\includegraphics[width=\textwidth]{images/speedratio_ML}
			\end{minipage}
		\captionof{figure}{Relative speed of our algorithm compared to Sinkhorn--Knopp, for various values of $\epsilon$}
		\end{tcolorbox}
	\end{minipage}
}



%----------------------------------------------------------------------------------------
%	REFERENCES
%----------------------------------------------------------------------------------------


\headerbox{References}{name=references,below=lyapunov,column=2,bottomaligned=overrelaxation}{
	\scriptsize
	\begin{multicols}{2}
		\textbf{[Cuturi, 2013]} M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. {\it Advances in Neural Information Processing Systems 26}, 2013.
		
		\textbf{[Benamou et al., 2015]} Benamou, J.-D., Carlier, G., Cuturi, M., Nenna, L., and Peyr\'e, G. (2015).
		{\it Iterative Bregman projections for regularized transportation problems.} SIAM Journal on Scientific
		Computing, 37(2):A1111‚ÄìA1138.
		
		\textbf{[Sinkhorn, 1964]} Sinkhorn, R. (1964). {\it A relationship between arbitrary positive matrices and doubly stochastic matrices.} The annals of mathematical statistics, 35(2):876‚Äì879.
		
		\textbf{[Schmitzer, 2016]} Schmitzer, B. 2016. Stabilized sparse scaling algorithms for entropy regularized transport problems. {\it arXiv preprint arXiv:1610.06519.}
	\end{multicols}
}


%----------------------------------------------------------------------------------------
%	Notations and definitions
%----------------------------------------------------------------------------------------

\headerbox{Sinkhorn-Knopp Algorithm}{name=Sinkhorn,column=0,below=intro,bottomaligned=expe}{
	We aim at numerically solving the entropic regularization of optimal transport:
	\begin{equation} \label{eq:problem}
	\gamma^* = \argmin_{\gamma \in \Ccal_1 \cap \Ccal_2}
	\scal{c}{\gamma} + \epsilon \KL(\gamma,\One).
	\end{equation}
	In this equation, $\Ccal_1$ and $\Ccal_2$ are the marginal constraint sets corresponding to two discrete measures $\mu^1$ and $\mu^2$,
	and where $\KL$ is the Kullback--Leibler divergence.
	\begin{gather*}
	\Ccal_1 = \left\{ \gamma \mid A_1 \gamma = \mu^1 \right\},
	\quad\quad\quad
	\Ccal_2 = \left\{ \gamma \mid A_2 \gamma = \mu^2 \right\}.\\
	\KL(\gamma,\xi) = \sum_{i,j} \gamma_{i,j} \left( \log \left( \frac{\gamma_{i,j}}{\xi_{i,j}} \right) -1  \right) + \sum_{i,j} \xi_{i,j}.
	\end{gather*}
	\begin{minipage}{0.55\textwidth}
		As problem (\ref{eq:problem}) is strictly convex, it has exactly one solution, that can be written equivalently as the \emph{Bregman projection} of $\gamma^0 = e^{-c/\epsilon}$ onto a convex set:
		\begin{gather*}
		P_{\Ccal}(\xi) := \argmin_{\gamma \in \Ccal} \KL(\gamma,\xi),\\
		\text{(\ref{eq:problem})} \quad \Longleftrightarrow \quad \gamma^* = P_{\Ccal_1 \cap \Ccal_2}(\gamma^0).
		\end{gather*}
		Whereas projecting onto $\Ccal_1 \cap \Ccal_2$ is challenging, projecting onto $\Ccal_1$ or $\Ccal_2$ individually is simply performed by scaling the rows or columns of the matrix so that the corresponding marginal sum is correct.
		Operator $\oslash$ denotes element-wise division:
		\begin{align*}\label{scaling}
		P_{\Ccal_1}(\gamma) &= \diag(a) \gamma &\text{with}\quad
		a &=  {\mu^1}\oslash{A_1 \gamma} \\
		P_{\Ccal_2}(\gamma) &= \gamma \diag(b) &\text{with}\quad
		b &= {\mu^2}\oslash{A_2 \gamma}\nonumber
		\end{align*}
	\end{minipage}
	\begin{minipage}{0.44\textwidth}
		\begin{tcolorbox}[/tcb/size=small, /tcb/top=2.0mm]
			\centering
			\input{schema_a}
			\captionof{figure}{The Sinkhorn--Knopp algorithm iterates Bregman projections on the two constraint sets.}
		\end{tcolorbox}
	\end{minipage}
	
	As shown in [Bregman, 1964], the solution can be computed by alternating Bregman projections. The main idea of the Sinkhorn--Knopp algorithm can therefore be written as:
	\begin{equation}
	\lim P_{\Ccal_2}\circ P_{\Ccal_1} \circ \ldots \circ P_{\Ccal_2} \circ P_{\Ccal_1} (\gamma^0) = \gamma^*
	\end{equation}
	This algorithm converges linearly: if we denote by $\gamma^\ell = (P_{\Ccal_2} \circ P_{\Ccal_1})^{(\ell)}(\gamma^0)$ the $\ell$-th iterate, then
	\[ \norm{\gamma^\ell - \gamma^*} = \mathcal{O}\left((1-\eta)^\ell \right) , \quad \eta > 0 \]
}



\end{poster}

\end{document}